{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Description\n",
    "----\n",
    "\n",
    "### Introduction to Challenge\n",
    "----\n",
    "- This coding test does not necessarily reflect or indicate what we are working on at Borgward R&D Silicon Valley.-  - The test is just to provide a way for us to estimate our candidateâ€™s coding skills\n",
    "- We are not looking for the exact correct answer or solutions, we are more looking for the way how you solve this problem, the way how you code, code style, code formality, code efficiencies, and most importantly the innovative thinking & ideas that you express in your algorithms. \n",
    "- Please do NOT spend more than 2 hours on this test, and try your best to finish the test with most efficient and practical code.\n",
    "\n",
    "### Problem\n",
    "----\n",
    "- BDD100K is a large-scale diverse driving video database: http://bair.berkeley.edu/blog/2018/05/30/bdd/. There are different tasks of perception algorithms such as object detection, semantic segmentations, lane detections that can be developed and tested using this dataset.\n",
    "- Let's now focus on one task called semantic segmentation. Semantic segmentation is so called \"dense\" prediction problem, where predictions are demanded at per pixel level. \n",
    "- Segmentation algorithms usually involves deep neural network, with approaches such as \"encoder-decoder\" architectures, \"dilated convolutional layers\", \"multi-scale receptive field\", so on and so forth.\n",
    "- We are not going to ask you to code a DNN model to perform semantic segmentation of course for an interview test. Instead, we are working some handy tools as shown below in the figure: we need some code or algorithms to draw a bounding box around \"car\" or \"vehicle pixels\", in the example figure below, we have two cars in the raw image on the left, and assuming a prefect semantic segmentation DNN can give you a semantic map as shown on the right, we want to detect the boundaries or bounding boxes for the two vehicles, white small vehicle on the left, and the red big SUV on the right.\n",
    "\n",
    "[image1]: ./image_rsrcs/problem.png \"Problem\"\n",
    "![Problem][image1]\n",
    "\n",
    "- Of course, this tasks is getting more and more challenging when vehicles have a lot of overlap on the image and shape becomes difficult to distinguish between different vehicles, but to get started we can assume the overlap is not huge.\n",
    "- You can use any programming language that you are comfortable with (c++, python preferred), and we'd prefer you code the algorithm yourself instead of calling a library such as OpenCV. \n",
    "- If you need some small sample data to test the code, we have a few images here from BDD100K: https://borgward.atlassian.net/wiki/spaces/BORPub/pages/655369/Public+Available+Dataset you can download the tar ball for the small sample dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution\n",
    "----\n",
    "\n",
    "\n",
    "### Semantic Segmentation on Kitti\n",
    "----\n",
    "- I have choosen Kitti Dataset for Vehicle Detection. you can find the description of dataset [here](http://www.cvlibs.net/datasets/kitti/eval_semseg.php?benchmark=semantics2015)\n",
    "- I have tried to replicate [Fully Convolution Network](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)\n",
    "- I have used a pretrained VGG16 Model as Encoder and in Decoder I have used Transpose Convolution with Skip Connections from the encoder\n",
    "- Download VGG16 from [here](http://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip)\n",
    "- Extract the contents of the VGG in the \"./data/vgg_model\" folder \n",
    "- This network is used for Semantic Segmentation whose architecture is below:\n",
    "[image2]: ./image_rsrcs/semantic_segmentation.png \"SemanticSegmentation\"\n",
    "![SemanticSegmentation][image2]\n",
    "\n",
    "- Training image and its corresponding Ground Truth: \n",
    "[image3]: ./image_rsrcs/kitti_data.png \"KittiData\"\n",
    "![KittiData][image3]\n",
    "\n",
    "\n",
    "### Semantic Segmentation Results\n",
    "----\n",
    "[image4]: ./image_rsrcs/KittiInference.png \"KittiInference\"\n",
    "![KittiInference][image4]\n",
    "\n",
    "[image5]: ./image_rsrcs/KittiInference2.png \"KittiInference2\"\n",
    "![KittiInference2][image5]\n",
    "\n",
    "\n",
    "[image6]: ./image_rsrcs/KittiInference3.png \"KittiInference3\"\n",
    "![KittiInference3][image6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection   \n",
    "----\n",
    "\n",
    "### Object Detection with Tensorflow API\n",
    "----\n",
    "- Used TensorFlow's Pretrained SSD model fro detecting Cars\n",
    "- This model is capable of performing object detecting in real time which is faster than Semantic Segmentation\n",
    "- Link to TensorFlow Object Detection Api is [here](https://github.com/tensorflow/models/tree/master/research/object_detection)\n",
    "- This model is used to detect cars on this data: https://borgward.atlassian.net/wiki/spaces/BORPub/pages/655369/Public+Available+Datase\n",
    "\n",
    "\n",
    "### Results\n",
    "----\n",
    "[image7]: ./image_rsrcs/carDetection1.png \"carDetection1\"\n",
    "![carDetection1][image7]\n",
    "\n",
    "[image8]: ./image_rsrcs/carDetection2.png \"carDetection2\"\n",
    "![carDetection2][image8]\n",
    "\n",
    "\n",
    "[image9]: ./image_rsrcs/carDetection3.png \"carDetection3\"\n",
    "![carDetection3][image9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
